{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed28608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class nyu_dataset():\n",
    "    def __init__(self,filename,type_of_data):\n",
    "        self.file=pd.read_csv(filename)\n",
    "        images_path=self.file[\"Images\"]\n",
    "        depths_path=self.file[\"Depth\"]\n",
    "        images_path_2=self.file[\"Images\"]\n",
    "        depths_path_2=self.file[\"Depth\"]\n",
    "        \n",
    "        \n",
    "        self.length=len(self.file)\n",
    "        ratio=0.75\n",
    "        training_set_size=int(ratio*self.length)\n",
    "        \n",
    "        if type_of_data==\"train\":\n",
    "            self.images_path=list(images_path[0:training_set_size])                 #3/4 is train data\n",
    "            self.depths_path=list(depths_path[0:training_set_size])\n",
    "        elif type_of_data==\"validation\":\n",
    "            self.images_path=list(images_path_2[training_set_size:])                  #1/4 is validation data\n",
    "            self.depths_path=list(depths_path_2[training_set_size:])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # print(self.images_path)\n",
    "        image_path=self.images_path[idx]\n",
    "        depth_path=self.depths_path[idx]\n",
    "        image=Image.open(image_path)\n",
    "        depth=Image.open(depth_path)\n",
    "        \n",
    "        transform_image = transforms.Compose([\n",
    "        transforms.Resize((228, 304)),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        transform_depth = transforms.Compose([\n",
    "        transforms.Resize((55, 74)),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        image_tensor=transform_image(image)\n",
    "        depth_tensor=transform_depth(depth)\n",
    "        depth_tensor = depth_tensor/1000-1\n",
    "        \n",
    "        item={'Image':image_tensor,'Depth':depth_tensor}\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ed148e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "output_height=55\n",
    "output_width=74\n",
    "epoch=10\n",
    "\n",
    "total_number_of_pixels=output_height*output_width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8432addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch=8\n",
    "#training using test data set because training dataset has large data and takes long time to train\n",
    "#just change test to train to use training dataset\n",
    "training_data_loading=torch.utils.data.DataLoader(nyu_dataset(\"nyu2_test.csv\",\"train\"),batch_size=size_of_batch)\n",
    "validation_data_loading=torch.utils.data.DataLoader(nyu_dataset(\"nyu2_test.csv\",\"validation\"),batch_size=size_of_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "977f2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class coarse_network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1=nn.Conv2d(in_channels=3,out_channels=96,kernel_size=(11,11),stride=4)\n",
    "        self.c2=nn.Conv2d(in_channels=96,out_channels=256,kernel_size=(5,5))\n",
    "        self.c3=nn.Conv2d(in_channels=256,out_channels=384,kernel_size=(3,3))\n",
    "        self.c4=nn.Conv2d(in_channels=384,out_channels=384,kernel_size=(3,3))\n",
    "        self.c5=nn.Conv2d(in_channels=384,out_channels=256,kernel_size=(3,3))   \n",
    "        self.fc1=nn.Linear(12800,4096)\n",
    "        self.fc2=nn.Linear(4096,4070)   #output should be 74*55=4070\n",
    "        self.pool=nn.MaxPool2d(2)\n",
    "        self.dropout=nn.Dropout() \n",
    "        self._init_weights(coarse_network)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self,module):                    #https://wandb.ai/wandb_fc/tips/reports/How-to-Initialize-Weights-in-PyTorch--VmlldzoxNjcwOTg1#:~:text=One%20of%20the%20most%20popular,in%20a%20custom%20PyTorch%20model.&text=This%20code%20snippet%20initializes%20all,all%20the%20biases%20to%20zero.\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                module.bias.data.fill(0.001)\n",
    "            elif isinstance(module,nn.Conv2d):\n",
    "                module.weight.data,normal(0,0.01)   #Because in imagenet classification it is used(mentioned in research paper) uses this initilization\n",
    "                module.bias.data.zero()\n",
    "\n",
    "    def forward(self,x):\n",
    "        #all layers are relu activated except layer 7 which is linear and there is a dropout after layer 6(mentioned in research paper)\n",
    "        x=F.relu(self.c1(x))\n",
    "        x=self.pool(x)\n",
    "        x=F.relu(self.c2(x))\n",
    "        x=self.pool(x)\n",
    "        x=F.relu(self.c3(x))\n",
    "        x=F.relu(self.c4(x))\n",
    "        x=F.relu(self.c5(x))\n",
    "        x=x.view(x.size(0),-1)  #we the next layer is a fully connected layer\n",
    "        x=F.relu(self.fc1(x))   #default value of p=0.5 (Bernouli distribution), kind of regularization\n",
    "        x=self.fc2(x)     \n",
    "        x=self.dropout(x)         #this layer have linear acativation (identity function)\n",
    "        x=x.view(-1,1,55,74)\n",
    "        return x\n",
    "        \n",
    "class fine_network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1=nn.Conv2d(in_channels=3,out_channels=63,kernel_size=(9,9),stride=2)\n",
    "        self.c2=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(5,5),padding=2)\n",
    "        self.c3=nn.Conv2d(in_channels=64,out_channels=1,kernel_size=(5,5),padding=2)\n",
    "        self.pool=nn.MaxPool2d(2)\n",
    "        self._init_weights(fine_network)   \n",
    "    \n",
    "    def _init_weights(self,module):                    #https://wandb.ai/wandb_fc/tips/reports/How-to-Initialize-Weights-in-PyTorch--VmlldzoxNjcwOTg1#:~:text=One%20of%20the%20most%20popular,in%20a%20custom%20PyTorch%20model.&text=This%20code%20snippet%20initializes%20all,all%20the%20biases%20to%20zero.\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                module.bias.data.fill(0.001)\n",
    "            elif isinstance(module,nn.Conv2d):\n",
    "                module.weight.data,normal(0,0.01)   #Because in imagenet classification it is used(mentioned in research paper) uses this initilization\n",
    "                module.bias.data.zero()\n",
    "            \n",
    "    def forward(self,x,y):\n",
    "        x=F.relu(self.c1(x))\n",
    "        x=self.pool(x)\n",
    "        x=torch.cat((x,y),1)\n",
    "        x=F.relu(self.c2(x))\n",
    "        x=self.c3(x)\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e91a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss_function(pred,actual):\n",
    "#     di=torch.log(pred)-torch.log(actual), log values are giving some undefined values so used without log and normalized the depth map\n",
    "    di=pred-actual\n",
    "    di_square=torch.pow(di,2)\n",
    "#     print(di)\n",
    "#     print(di_square.shape)\n",
    "    n=total_number_of_pixels\n",
    "    loss=(torch.sum(di_square,(1,2,3))/n)-(0.5)*(torch.pow(torch.sum(di,(1,2,3)),2)/(n*n))\n",
    "#     print(loss)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "93563ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_invariant_loss_function(pred,actual):  #same as training loss except the factor lambda=0.5\n",
    "#     di=torch.log(pred)-torch.log(actual)\n",
    "    di=pred-actual\n",
    "    di_square=torch.pow(di,2)\n",
    "    n=total_number_of_pixels\n",
    "    loss=(torch.sum(di_square,(1,2,3))/n)-(torch.pow(torch.sum(di,(1,2,3)),2)/(n*n))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d917264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizers\n",
    "coarse_model=coarse_network()\n",
    "fine_model=fine_network()\n",
    "coarse_network_optimizer=optim.Adam(coarse_model.parameters(),lr=0.001, betas=(0.9, 0.999), eps=1e-08)  #from official documentation of adam optimizer\n",
    "fine_network_optimizer=optim.Adam(fine_model.parameters(),lr=0.001, betas=(0.9, 0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5af10c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_coarse_network(coarse_model,training_data_loading,coarse_network_optimizer):\n",
    "    coarse_model.train()\n",
    "    training_coarse_loss=0\n",
    "    for i, batch in enumerate(training_data_loading):\n",
    "        image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=True)\n",
    "        depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=True)\n",
    "        coarse_network_optimizer.zero_grad()             #Sets the gradients of all optimized torch.Tensors to zero\n",
    "        pred=coarse_network().forward(image)\n",
    "        actual=depth\n",
    "        loss=training_loss_function(pred,actual)\n",
    "        loss.backward()\n",
    "        coarse_network_optimizer.step()\n",
    "        training_coarse_loss+=loss.item()                #we need as a standard python number of item is used\n",
    "    training_coarse_loss=(training_coarse_loss/(i+1))        #taking avg over batches\n",
    "    return training_coarse_loss\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "abade802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_fine_network(coarse_model,fine_model,training_data_loading,fine_network_optimizer):\n",
    "    fine_model.train()\n",
    "    coarse_model.eval()\n",
    "    training_fine_loss=0\n",
    "    for i, batch in enumerate(training_data_loading):\n",
    "        image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=True)\n",
    "        depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=True)\n",
    "        fine_network_optimizer.zero_grad()            #Sets the gradients of all optimized torch.Tensors to zero\n",
    "        coarse_network_output=coarse_network().forward(image)\n",
    "        pred=fine_network().forward(image,coarse_network_output)\n",
    "        actual=depth\n",
    "        loss=training_loss_function(pred,actual)\n",
    "        loss.backward()\n",
    "        fine_network_optimizer.step()\n",
    "        training_fine_loss+=loss.item()                 #we need as a standard python number of item is used\n",
    "    training_fine_loss=(training_fine_loss/(i+1))           #taking avg over batches\n",
    "    return training_fine_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a209976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_network_validation(coarse_model,validation_data_loading):\n",
    "    coarse_model.eval()\n",
    "    coarse_validation_loss=0\n",
    "    scale_invariant_loss=0\n",
    "    for i, batch in enumerate(validation_data_loading):\n",
    "        image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=False)\n",
    "        depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=False)\n",
    "        pred=coarse_network().forward(image)\n",
    "        actual=depth\n",
    "        coarse_validation_loss+=training_loss_function(pred,actual).item()\n",
    "        scale_invariant_loss+=scale_invariant_loss_function(pred,actual).item()\n",
    "    coarse_validation_loss=(coarse_validation_loss/(i+1))\n",
    "    scale_invariant_loss=(scale_invariant_loss/(i+1))\n",
    "    loss=[coarse_validation_loss,scale_invariant_loss]\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "013c56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_network_validation(fine_model,validation_data_loading):\n",
    "    fine_model.eval()\n",
    "    fine_validation_loss=0\n",
    "    scale_invariant_loss=0\n",
    "    for i, batch in enumerate(validation_data_loading):\n",
    "        image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=False)\n",
    "        depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=False)\n",
    "        coarse_network_output=coarse_network().forward(image)\n",
    "        pred=fine_network().forward(image,coarse_network_output)\n",
    "        actual=depth\n",
    "        fine_validation_loss+=training_loss_function(pred,actual).item()\n",
    "        scale_invariant_loss+=scale_invariant_loss_function(pred,actual).item()\n",
    "    fine_validation_loss=(fine_validation_loss/(i+1))\n",
    "    scale_invariant_loss=(scale_invariant_loss/(i+1))\n",
    "    loss=[fine_validation_loss,scale_invariant_loss]\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1b94c52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-67-3f3550b53d0f>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=True)\n",
      "<ipython-input-67-3f3550b53d0f>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=True)\n",
      "<ipython-input-69-31e338b6897a>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [2.634761837220961, [4.098639153298878, 1.6470668741634913]], 1: [2.634897409908233, [4.098531172389076, 1.6469523466768718]], 2: [2.634840085621803, [4.098591861270723, 1.6470208494436174]], 3: [2.6348747236113392, [4.09861637864794, 1.6470301236425127]], 4: [2.634872505741735, [4.098510305086772, 1.6470812672660464]], 5: [2.6347786370785005, [4.098704570815677, 1.647116822855813]], 6: [2.6348652248421023, [4.098535770461673, 1.6470766039121718]], 7: [2.634898086228678, [4.098387774967012, 1.6469320371037437]], 8: [2.6349206326469297, [4.0986424798057195, 1.6470782274291629]], 9: [2.6348916638282036, [4.098745698020572, 1.6470425497917902]]}\n"
     ]
    }
   ],
   "source": [
    "coarse_losses_after_each_epoch={}                 # {training loss: [validation loss,scale_invariant_loss]}\n",
    "for i in range(epoch):\n",
    "    coarse_training_loss=training_coarse_network(coarse_model,training_data_loading,coarse_network_optimizer)\n",
    "    coarse_validation_loss=coarse_network_validation(coarse_model,validation_data_loading)\n",
    "    coarse_losses_after_each_epoch[i]=[coarse_training_loss,coarse_validation_loss]\n",
    "print(coarse_losses_after_each_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "898d8ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-31c4ad8ce911>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=True)\n",
      "<ipython-input-68-31c4ad8ce911>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [2.6317163369347973, [4.079009061767941, 1.6469448393299466]], 1: [2.631667755303844, [4.113340911411104, 1.6480932505357833]], 2: [2.674490776273512, [4.114491797628856, 1.6497645747093928]], 3: [2.654037401560814, [4.120282184510004, 1.6492914387157984]], 4: [2.617448898092393, [4.096834443864369, 1.6468416580132075]], 5: [2.653712598066176, [4.061759239151364, 1.6438066987764268]], 6: [2.650643456847437, [4.088529785474141, 1.644479391120729]], 7: [2.619757235530884, [4.075116464069912, 1.6445517979917073]], 8: [2.635171913331555, [4.130638185001555, 1.6519657671451569]], 9: [2.6428898558501275, [4.056015281450181, 1.641001162074861]]}\n"
     ]
    }
   ],
   "source": [
    "fine_losses_after_each_epoch={}       # {training loss: [validation loss,scale_invariant_loss]}\n",
    "for i in range(epoch):\n",
    "    fine_training_loss=training_fine_network(coarse_model,fine_model,training_data_loading,fine_network_optimizer)\n",
    "    fine_validation_loss=fine_network_validation(fine_model,validation_data_loading)\n",
    "    fine_losses_after_each_epoch[i]=[fine_training_loss,fine_validation_loss]\n",
    "print(fine_losses_after_each_epoch)          #we can draw graph for each epoch values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5cc77bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-87-8815e0243100>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=False)\n",
      "<ipython-input-87-8815e0243100>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "#we can use this code to see the predicition image, here i am using validation set only\n",
    "for i, batch in enumerate(validation_data_loading):\n",
    "    image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=False)\n",
    "    depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=False)\n",
    "    coarse_network_output=coarse_network().forward(image)\n",
    "    pred=fine_network().forward(image,coarse_network_output)\n",
    "    tensor=pred[1]\n",
    "transform = transforms.ToPILImage()\n",
    "img=transform(tensor)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597a632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
