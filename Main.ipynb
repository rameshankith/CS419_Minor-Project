{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "ed28608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class nyu_dataset():\n",
    "    def __init__(self,filename,type_of_data):\n",
    "        self.file=pd.read_csv(filename)\n",
    "        images_path=self.file[\"Images\"]\n",
    "        depths_path=self.file[\"Depth\"]\n",
    "        images_path_2=self.file[\"Images\"]\n",
    "        depths_path_2=self.file[\"Depth\"]\n",
    "        \n",
    "        \n",
    "        self.length=len(self.file)\n",
    "        ratio=0.75\n",
    "        training_set_size=int(ratio*self.length)\n",
    "        \n",
    "        if type_of_data==\"train\":\n",
    "            self.images_path=images_path[0:training_set_size]                 #3/4 is train data\n",
    "            self.depths_path=depths_path[0:training_set_size]\n",
    "        elif type_of_data==\"validation\":\n",
    "            self.images_path=images_path_2[training_set_size:]                  #1/4 is validation data\n",
    "            self.depths_path=depths_path_2[training_set_size:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image_path=self.images_path[idx]\n",
    "        depth_path=self.depths_path[idx]\n",
    "        image=Image.open(image_path)\n",
    "        depth=Image.open(depth_path)\n",
    "        \n",
    "        transform_image = transforms.Compose([\n",
    "        transforms.Resize((228, 304)),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        transform_depth = transforms.Compose([\n",
    "        transforms.Resize((55, 74)),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        image_tensor=transform_image(image)\n",
    "        depth_tensor=transform_depth(depth)\n",
    "        \n",
    "        item={'Image':image_tensor,'Depth':depth_tensor}\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ed148e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "output_height=55\n",
    "output_width=74\n",
    "epoch=10\n",
    "\n",
    "total_number_of_pixels=output_height*output_width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "8432addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch=8\n",
    "\n",
    "training_data_loading=torch.utils.data.DataLoader(nyu_dataset(\"nyu2_test.csv\",\"train\"),batch_size=size_of_batch)\n",
    "validation_data_loading=torch.utils.data.DataLoader(nyu_dataset(\"nyu2_test.csv\",\"validation\"),batch_size=size_of_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "977f2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class coarse_network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1=nn.Conv2d(in_channels=3,out_channels=96,kernel_size=(11,11),stride=4)\n",
    "        self.c2=nn.Conv2d(in_channels=96,out_channels=256,kernel_size=(5,5))\n",
    "        self.c3=nn.Conv2d(in_channels=256,out_channels=384,kernel_size=(3,3))\n",
    "        self.c4=nn.Conv2d(in_channels=384,out_channels=384,kernel_size=(3,3))\n",
    "        self.c5=nn.Conv2d(in_channels=384,out_channels=256,kernel_size=(3,3))   \n",
    "        self.fc1=nn.Linear(12800,4096)\n",
    "        self.fc2=nn.Linear(4096,4070)   #output should be 74*55=4070\n",
    "        self.pool=nn.MaxPool2d(2)\n",
    "        self.dropout=nn.Dropout() \n",
    "        self._init_weights(coarse_network)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self,module):                    #https://wandb.ai/wandb_fc/tips/reports/How-to-Initialize-Weights-in-PyTorch--VmlldzoxNjcwOTg1#:~:text=One%20of%20the%20most%20popular,in%20a%20custom%20PyTorch%20model.&text=This%20code%20snippet%20initializes%20all,all%20the%20biases%20to%20zero.\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                module.bias.data.fill(0.001)\n",
    "            elif isinstance(module,nn.Conv2d):\n",
    "                module.weight.data,normal(0,0.01)   #Because in imagenet classification it is used(mentioned in research paper) uses this initilization\n",
    "                module.bias.data.zero()\n",
    "\n",
    "    def forward(self,x):\n",
    "        #all layers are relu activated except layer 7 which is linear and there is a dropout after layer 6(mentioned in research paper)\n",
    "        x=F.relu(self.c1(x))\n",
    "        x=self.pool(x)\n",
    "        x=F.relu(self.c2(x))\n",
    "        x=self.pool(x)\n",
    "        x=F.relu(self.c3(x))\n",
    "        x=F.relu(self.c4(x))\n",
    "        x=F.relu(self.c5(x))\n",
    "        x=x.view(x.size(0),-1)  #we the next layer is a fully connected layer\n",
    "        x=F.relu(self.fc1(x))   #default value of p=0.5 (Bernouli distribution), kind of regularization\n",
    "        x=self.fc2(x)     \n",
    "        x=self.dropout(x)         #this layer have linear acativation (identity function)\n",
    "        x=x.view(-1,1,55,74)\n",
    "        return x\n",
    "        \n",
    "class fine_network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1=nn.Conv2d(in_channels=3,out_channels=63,kernel_size=(9,9),stride=2)\n",
    "        self.c2=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(5,5),padding=2)\n",
    "        self.c3=nn.Conv2d(in_channels=64,out_channels=1,kernel_size=(5,5),padding=2)\n",
    "        self.pool=nn.MaxPool2d(2)\n",
    "        self._init_weights(fine_network)   \n",
    "    \n",
    "    def _init_weights(self,module):                    #https://wandb.ai/wandb_fc/tips/reports/How-to-Initialize-Weights-in-PyTorch--VmlldzoxNjcwOTg1#:~:text=One%20of%20the%20most%20popular,in%20a%20custom%20PyTorch%20model.&text=This%20code%20snippet%20initializes%20all,all%20the%20biases%20to%20zero.\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                module.bias.data.fill(0.001)\n",
    "            elif isinstance(module,nn.Conv2d):\n",
    "                module.weight.data,normal(0,0.01)   #Because in imagenet classification it is used(mentioned in research paper) uses this initilization\n",
    "                module.bias.data.zero()\n",
    "            \n",
    "    def forward(self,x,y):\n",
    "        x=F.relu(self.c1(x))\n",
    "        x=self.pool(x)\n",
    "        x=torch.cat((x,y),1)\n",
    "        x=F.relu(self.c2(x))\n",
    "        x=self.c3(x)\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "3e91a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss_function(pred,actual):\n",
    "    di=torch.log(pred)-torch.log(actual)\n",
    "    di_square=torch.pow(di,2)\n",
    "    n=total_number_of_pixels\n",
    "    loss=(torch.sum(di_square,(1,2,3))/n)-(0.5)*(torch.pow(torch.sum(di,(1,2,3)),2)/(n*n))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "93563ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_invariant_loss_function(pred,actual):  #same as training loss except the factor lambda=0.5\n",
    "    di=torch.log(pred)-torch.log(actual)\n",
    "    di_square=torch.pow(di,2)\n",
    "    n=total_number_of_pixels\n",
    "    loss=(torch.sum(di_square,(1,2,3))/n)-(torch.pow(torch.sum(di,(1,2,3)),2)/(n*n))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "0d917264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizers\n",
    "coarse_model=coarse_network()\n",
    "fine_model=fine_network()\n",
    "coarse_network_optimizer=optim.Adam(coarse_model.parameters(),lr=0.001, betas=(0.9, 0.999), eps=1e-08)  #from official documentation of adam optimizer\n",
    "fine_network_optimizer=optim.Adam(fine_model.parameters(),lr=0.001, betas=(0.9, 0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "5af10c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_coarse_network(coarse_model,training_data_loading,coarse_network_optimizer):\n",
    "    coarse_model.train()\n",
    "    training_coarse_loss=0\n",
    "    for i, batch in enumerate(training_data_loading):\n",
    "        image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=True)\n",
    "        depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=True)\n",
    "        coarse_network_optimizer.zero_grad()             #Sets the gradients of all optimized torch.Tensors to zero\n",
    "        pred=coarse_network().forward(image)\n",
    "        actual=depth\n",
    "        loss=training_loss_function(pred,actual)\n",
    "        loss.backward()\n",
    "        coarse_network_optimizer.step()\n",
    "        training_coarse_loss+=loss.item()                #we need as a standard python number of item is used\n",
    "    training_coarse_loss=(training_coarse_loss/(i+1))        #taking avg over batches\n",
    "    return training_coarse_loss\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "abade802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_fine_network(coarse_model,fine_model,training_data_loading,fine_network_optimizer):\n",
    "    fine_model.train()\n",
    "    coarse_model.eval()\n",
    "    training_fine_loss=0\n",
    "    for i, batch in enumerate(training_data_loading):\n",
    "        image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=True)\n",
    "        depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=True)\n",
    "        fine_network_optimizer.zero_grad()            #Sets the gradients of all optimized torch.Tensors to zero\n",
    "        coarse_network_output=coarse_network().forward(image)\n",
    "        pred=fine_network().forward(image,coarse_network_output)\n",
    "        actual=depth\n",
    "        loss=training_loss_function(pred,actual)\n",
    "        loss.backward()\n",
    "        fine_network_optimizer.step()\n",
    "        training_fine_loss+=loss.item()                 #we need as a standard python number of item is used\n",
    "    training_fine_loss=(training_fine_loss/(i+1))           #taking avg over batches\n",
    "    return training_fine_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "a209976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_network_validation(coarse_model,validation_data_loading):\n",
    "    coarse_model.eval()\n",
    "    coarse_validation_loss=0\n",
    "    scale_invariant_loss=0\n",
    "    for i, batch in enumerate(validation_data_loading):\n",
    "        image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=False)\n",
    "        depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=False)\n",
    "        pred=coarse_network().forward(image)\n",
    "        actual=depth\n",
    "        cross_validation_loss+=training_loss_function(pred,actual).item()\n",
    "        scale_invariant_loss+=scale_invariant_loss_function(pred,actual).item()\n",
    "    cross_validation_loss=(cross_validation_loss/(i+1))\n",
    "    scale_invariant_loss=(scale_invariant_loss/(i+1))\n",
    "    loss=[cross_validation_loss,scale_invariant_loss]\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "013c56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_network_validation(fine_model,validation_data_loading):\n",
    "    fine_model.eval()\n",
    "    fine_validation_loss=0\n",
    "    scale_variant_loss=0\n",
    "    for i, batch in enumerate(validation_data_loading):\n",
    "        image=torch.tensor(batch['Image'].type(torch.FloatTensor),requires_grad=False)\n",
    "        depth=torch.tensor(batch['Depth'].type(torch.FloatTensor),requires_grad=False)\n",
    "        coarse_network_output=coarse_network().forward(image)\n",
    "        pred=fine_network().forward(image,coarse_network_output)\n",
    "        actual=depth\n",
    "        cross_validation_loss+=training_loss_function(pred,actual).item()\n",
    "        scale_invariant_loss+=scale_invariant_loss_function(pred,actual).item()\n",
    "    cross_validation_loss=(cross_validation_loss/(i+1))\n",
    "    scale_invariant_loss=(scale_invariant_loss/(i+1))\n",
    "    loss=[cross_validation_loss,scale_invariant_loss]\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    coarse_losses_after_each_epoch={}                 # {training loss: [validation loss,scale_invariant_loss]}\n",
    "    coarse_training_loss=training_coarse_network(coarse_model,training_data_loading,coarse_network_optimizer)\n",
    "    coarse_validation_loss=coarse_network_validation(coarse_model,validation_data_loading)\n",
    "    coarse_losses_after_each_epoch[coarse_training_loss]=coarse_validation_loss\n",
    "print(coarse_losses_after_each_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    fine_losses_after_each_epoch={}       # {training loss: [validation loss,scale_invariant_loss]}\n",
    "    fine_training_loss=training_fine_network(coarse_model,fine_model,training_data_loading,fine_network_optimizer)\n",
    "    fine_validation_loss=fine_network_validation(fine_model,validation_data_loading)\n",
    "    fine_losses_after_each_epoch[fine_training_loss]=fine_validation_loss\n",
    "print(fine_losses_after_each_epoch)          #we can draw graph for each epoch values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor to image conversion\n",
    "image = Image.open(image_path)\n",
    "coarse_network_output=coarse_network(image)\n",
    "tensor = fine_network(image,coarse_network_output)\n",
    "img = transforms.ToPILImage(tensor)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597a632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
